# pipelines.py

import json
import requests
from typing import Optional
from llama_index.core import load_index_from_storage, StorageContext, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import warnings
import re

warnings.filterwarnings("ignore", category=FutureWarning, module="sentence_transformers.SentenceTransformer")

# --- SHARED SETUP ---

print("--- Setting up LlamaIndex models ---")
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5")
Settings.llm = None # We are using the DeepSeek API directly

print("--- Loading indexes from storage ---")
try:
    entity_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./storage/entity_index"))
    class_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./storage/class_index"))
    prop_index = load_index_from_storage(StorageContext.from_defaults(persist_dir="./storage/prop_index"))
except FileNotFoundError:
    raise FileNotFoundError("Error: Could not load indexes. Please run 'build_indexes.py' first.")

# Create a retriever for each index
entity_retriever = entity_index.as_retriever(similarity_top_k=5)
class_retriever = class_index.as_retriever(similarity_top_k=5)
prop_retriever = prop_index.as_retriever(similarity_top_k=5)

# --- TOOL DEFINITIONS (Python functions the agent can call) ---
def search_for_entity(query: str):
    """Searches the knowledge graph for specific named entities like people, places, or items."""
    nodes = entity_retriever.retrieve(query)
    return json.dumps([{"name": n.metadata['name'], "uri": n.metadata['uri']} for n in nodes])

def search_for_class(query: str):
    """Searches the knowledge graph for categories or types of things, like 'Witchers' or 'Cities'."""
    nodes = class_retriever.retrieve(query)
    return json.dumps([{"name": n.metadata['name'], "uri": n.metadata['uri']} for n in nodes])

def search_for_property(query: str):
    """Searches the knowledge graph for attributes or relationships, like 'hair color' or 'affiliations'."""
    nodes = prop_retriever.retrieve(query)
    return json.dumps([{"name": n.metadata['name'], "uri": n.metadata['uri']} for n in nodes])


def extract_sparql_from_llm_response(text: str) -> str:
    """
    Intelligently extracts a SPARQL query from a larger block of text
    generated by an LLM.
    """
    if not text:
        return ""

    # First, remove markdown backticks if they exist
    cleaned_text = text.replace("```sparql", "").replace("```", "").strip()
    
    # Find the start of the query (SELECT or ASK), case-insensitive
    select_match = re.search(r'SELECT', cleaned_text, re.IGNORECASE)
    ask_match = re.search(r'ASK', cleaned_text, re.IGNORECASE)

    start_pos = -1
    if select_match:
        start_pos = select_match.start()
    elif ask_match:
        start_pos = ask_match.start()

    # If a keyword was found, return the substring from that point onwards
    if start_pos != -1:
        return cleaned_text[start_pos:].strip()
    
    # If no keyword is found, return the cleaned text as a last resort
    return cleaned_text


# --- PIPELINE A: SIMPLE RETRIEVE-AND-SYNTHESIZE ---

class SimpleRAGPipeline:
    def __init__(self, api_key: str, model: str = "deepseek-chat"):
        self.api_key = api_key
        self.model = model
        self.api_url = "https://api.deepseek.com/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.system_prompt = """
        You are an expert SPARQL/GeoSPARQL query generator. Your task is to convert a user's question into a valid SPARQL query for a Witcher 3 knowledge graph.
        You will be provided with the user's question and a context of potentially relevant entities, classes, and properties retrieved from the graph.
        Use the context to find the correct URIs and property names.
        Only output the final, complete SPARQL query. Do not include any explanations, markdown, or other text.
        """

    def generate_query(self, question: str) -> str:
        entity_nodes = entity_retriever.retrieve(question)
        class_nodes = class_retriever.retrieve(question)
        prop_nodes = prop_retriever.retrieve(question)

        context_str = "--- Retrieved Entities ---\n"
        for node in entity_nodes:
            context_str += f"- Name: {node.metadata['name']}, URI: <{node.metadata['uri']}>\n"
        
        context_str += "\n--- Retrieved Classes ---\n"
        for node in class_nodes:
            context_str += f"- Name: {node.metadata['name']}, URI: <{node.metadata['uri']}>\n"
            
        context_str += "\n--- Retrieved Properties ---\n"
        for node in prop_nodes:
            context_str += f"- Name: {node.metadata['name']}, URI: <{node.metadata['uri']}>\n"

        user_prompt = f"""
        User Question: "{question}"

        Retrieved Context:
        {context_str}

        Based on the question and the context, generate the SPARQL query.
        """
        
        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": 1024,
            "temperature": 0.1
        }
        
        response = requests.post(self.api_url, headers=self.headers, json=data)
        response.raise_for_status()
        result = response.json()
        raw_llm_output = result["choices"][0]["message"]["content"]
        return extract_sparql_from_llm_response(raw_llm_output)

# --- PIPELINE B: AGENTIC FUNCTION CALLING (GRASP-inspired) ---

class AgenticRAGPipeline:
    def __init__(self, api_key: str, model: str = "deepseek-chat"):
        self.api_key = api_key
        self.model = model
        self.api_url = "https://api.deepseek.com/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.available_tools = {
            "search_for_entity": search_for_entity,
            "search_for_class": search_for_class,
            "search_for_property": search_for_property
        }
        self.system_prompt = """
        You are a reasoning agent that converts a user's question into a SPARQL/GeoSPARQL query.
        Your goal is to gather enough information to write the query.
        You have access to three tools to search a knowledge graph:
        1. search_for_entity(query): To find specific people, places, monsters, etc.
        2. search_for_class(query): To find types or categories of things.
        3. search_for_property(query): To find attributes or relationships.
        
        Follow this process:
        1. Analyze the user's question to identify the key entities, classes, and properties.
        2. Use the tools one by one to find the correct URIs for these components.
        3. After each tool call, review the results.
        4. Once you have gathered all necessary URIs, state that you are ready and then output the final, complete SPARQL query in a single block. Do not ask for permission.
        """
        self.tool_definitions = [
            {
                "type": "function",
                "function": {
                    "name": "search_for_entity",
                    "description": "Searches for specific named entities (people, places, items).",
                    "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "The name of the entity to search for, e.g., 'Geralt of Rivia'"}}, "required": ["query"]},
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_for_class",
                    "description": "Searches for a category or type of thing, e.g., 'Witcher', 'City'.",
                    "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "The name of the class to search for"}}, "required": ["query"]},
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_for_property",
                    "description": "Searches for an attribute or relationship, e.g., 'hair color', 'location'.",
                    "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "The name of the property to search for"}}, "required": ["query"]},
                }
            }
        ]

    def generate_query(self, question: str, max_steps: int = 5) -> str:
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": question}
        ]
        
        for _ in range(max_steps):
            data = {
                "model": self.model,
                "messages": messages,
                "tools": self.tool_definitions,
                "tool_choice": "auto"
            }
            
            response = requests.post(self.api_url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            response_message = result["choices"][0]["message"]
            
            # Append the assistant's entire message (which may include tool calls) to the history
            messages.append(response_message)
            
            if response_message.get("tool_calls"):
                for tool_call in response_message["tool_calls"]:
                    function_name = tool_call['function']['name']
                    function_args = json.loads(tool_call['function']['arguments'])
                    
                    # Call the actual Python function
                    function_response = self.available_tools[function_name](query=function_args.get("query"))
                    
                    # Append the tool's response in the correct format for the next API call
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call['id'],
                        "content": function_response,
                    })
            else:
                # If there are no tool calls, the model has given its final text answer
                final_content = response_message.get('content')
                if final_content and ("SELECT" in final_content or "ASK" in final_content):
                    return extract_sparql_from_llm_response(final_content)
                # If it's just a thought, the loop will continue with the updated message history
        
        # Fallback if the loop finishes without generating a query
        messages.append({"role": "user", "content": "You have now gathered all the information. Please generate the final SPARQL query based on our conversation."})
        final_data = {"model": self.model, "messages": messages}
        final_response = requests.post(self.api_url, headers=self.headers, json=final_data)
        final_response.raise_for_status()
        final_result = final_response.json()
        final_query_raw = final_result["choices"][0]["message"]["content"]
        return extract_sparql_from_llm_response(final_query_raw)