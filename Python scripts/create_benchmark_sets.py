import json
import argparse
import random
from collections import defaultdict

def main():
    parser = argparse.ArgumentParser(
        description="Create final validation and test sets from a generated benchmark and a translated sample.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "--full-benchmark-file", 
        default="witcher_benchmark_dataset_final_v7.json",
        help="The large JSON file generated by the main script."
    )
    parser.add_argument(
        "--translated-file", 
        default="translation_comparison_final.json",
        help="The smaller JSON file containing the LLM-translated queries."
    )
    parser.add_argument(
        "--max-per-template", 
        type=int, 
        default=40,
        help="The maximum total number of queries to keep for each template."
    )
    
    args = parser.parse_args()
    
    target_per_set = args.max_per_template // 2
    print(f"Targeting {target_per_set} queries per template for the validation set and {target_per_set} for the test set.")

    # --- 1. Load All Data ---
    print(f"Loading full benchmark from '{args.full_benchmark_file}'...")
    try:
        with open(args.full_benchmark_file, 'r') as f:
            all_queries = json.load(f)
    except FileNotFoundError:
        print(f"Error: Full benchmark file not found at '{args.full_benchmark_file}'")
        return

    print(f"Loading translated queries from '{args.translated_file}'...")
    try:
        with open(args.translated_file, 'r') as f:
            translated_queries = json.load(f)
    except FileNotFoundError:
        print(f"Error: Translated queries file not found at '{args.translated_file}'")
        return

    # --- 2. Process and Prioritize Translated Queries ---
    translated_ids = {item['query_id'] for item in translated_queries}
    must_include_by_template = defaultdict(list)
    
    for item in translated_queries:
        # Use the direct, one-shot translation as the final NLQ
        final_nlq = item.get('direct_llm_translation', item.get('template_generated_nlq'))
        if "ERROR:" in final_nlq:
            print(f"Warning: Skipping translated query {item['query_id']} due to translation error.")
            continue
            
        clean_item = {
            "query_id": item['query_id'],
            "template_id": item['template_id'],
            "sparql_query": item['sparql_query'],
            "natural_language_question": final_nlq
        }
        must_include_by_template[item['template_id']].append(clean_item)

    print(f"Processed {len(translated_queries)} translated queries to prioritize.")

    # --- 3. Group Remaining Queries ---
    remaining_by_template = defaultdict(list)
    for item in all_queries:
        if item['query_id'] not in translated_ids:
            clean_item = {
                "query_id": item['query_id'],
                "template_id": item['template_id'],
                "sparql_query": item['query'],
                "natural_language_question": item['natural_language_question']
            }
            remaining_by_template[item['template_id']].append(clean_item)

    # --- 4. Build the Validation and Test Sets ---
    validation_set = []
    test_set = []

    # Get all unique template IDs from both sources
    all_template_ids = sorted(list(set(must_include_by_template.keys()) | set(remaining_by_template.keys())))

    print("\nBuilding final sets...")
    for template_id in all_template_ids:
        must_include = must_include_by_template[template_id]
        random.shuffle(must_include)
        
        remaining = remaining_by_template[template_id]
        random.shuffle(remaining)

        val_for_template = []
        test_for_template = []

        # Distribute the prioritized "must-include" queries evenly
        for i, item in enumerate(must_include):
            if i % 2 == 0:
                val_for_template.append(item)
            else:
                test_for_template.append(item)
        
        # Top up the validation set with remaining queries until the target is met
        while len(val_for_template) < target_per_set and remaining:
            val_for_template.append(remaining.pop())
            
        # Top up the test set with remaining queries until the target is met
        while len(test_for_template) < target_per_set and remaining:
            test_for_template.append(remaining.pop())
            
        validation_set.extend(val_for_template)
        test_set.extend(test_for_template)
        print(f"  - Template {template_id}: Added {len(val_for_template)} to validation, {len(test_for_template)} to test.")

    # --- 5. Save the Final Sets ---
    print("\n--- Final Benchmark Summary ---")
    print(f"Total Validation Set size: {len(validation_set)} queries.")
    print(f"Total Test Set size: {len(test_set)} queries.")

    with open("validation_set.json", 'w') as f:
        json.dump(validation_set, f, indent=2)
    print("Saved validation_set.json")

    with open("test_set.json", 'w') as f:
        json.dump(test_set, f, indent=2)
    print("Saved test_set.json")

    print("\nProcess complete.")

if __name__ == "__main__":
    main()